{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import TUDataset, Entities, Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import site\n",
    "\n",
    "site.addsitedir(\"../\")\n",
    "\n",
    "from models import mlp, gcn, graphsage, graphsaint, gat, graphmlp\n",
    "from utils import config_utils as cfg_u\n",
    "from utils import model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.dgl.ai/dataset/aifb.tgz\n",
      "Extracting data/aifb.tgz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Extracting data/MUTAG/MUTAG.zip\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Load data!\")\n",
    "aifb_dataset = Entities(root=\"data\", name=\"AIFB\", transform=NormalizeFeatures())\n",
    "mutag_dataset = TUDataset(root=\"data\", name=\"MUTAG\", transform=NormalizeFeatures())\n",
    "dataset = Planetoid(root=\"data\", name=\"Cora\", transform=NormalizeFeatures())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data, criterion=torch.nn.CrossEntropyLoss()):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(model, data, mask):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def loop(model, optimizer, silent=False):\n",
    "    for data in dataset:\n",
    "        for epoch in range(1, 201):\n",
    "            loss = train(model, optimizer, data)\n",
    "            val_acc = test(model, data, data.val_mask)\n",
    "            test_acc = test(model, data, data.test_mask)\n",
    "            if epoch % 10 == 0 and not silent:\n",
    "                print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "            \n",
    "        test_acc = test(model, data, data.test_mask)\n",
    "        print(f'Test Accuracy for {model.name}: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model = gcn.GCN(num_classes=dataset.num_classes, num_features=dataset.num_features)\n",
    "gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "graphsage_model = graphsage.SAGE(num_classes=dataset.num_classes, num_features=dataset.num_features)\n",
    "graphsage_optimizer = torch.optim.Adam(graphsage_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "graphsaint_model = graphsaint.SAINT(num_classes=dataset.num_classes, num_node_features=dataset.num_node_features)\n",
    "graphsaint_optimizer = torch.optim.Adam(graphsaint_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "gat_model = gat.GAT(num_classes=dataset.num_classes, num_features=dataset.num_features)\n",
    "gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "graphmlp_model = graphmlp.GMLP(num_classes=dataset.num_classes, num_features=dataset.num_features)\n",
    "graphmlp_optimizer = torch.optim.Adam(graphmlp_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "mlp_model = mlp.MLP(num_classes=dataset.num_classes, num_features=dataset.num_features)\n",
    "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for MLP: 0.6000\n",
      "Test Accuracy for GCN: 0.8080\n",
      "Test Accuracy for GraphSAGE: 0.8040\n",
      "Test Accuracy for GraphSAINT: 0.7620\n",
      "Test Accuracy for GAT: 0.8220\n"
     ]
    }
   ],
   "source": [
    "loop(mlp_model, mlp_optimizer, silent=True)\n",
    "# loop(graphmlp_model, graphmlp_optimizer, silent=True)\n",
    "loop(gcn_model, gcn_optimizer, silent=True)\n",
    "loop(graphsage_model, graphsage_optimizer, silent=True)\n",
    "loop(graphsaint_model, graphsaint_optimizer, silent=True)\n",
    "loop(gat_model, gat_optimizer, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning Rate: 0.001, Hidden Channels: 32\n",
      "\n",
      "Test Accuracy for GCN: 0.8020\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Learning Rate: 0.001, Hidden Channels: 64\n",
      "\n",
      "Test Accuracy for GCN: 0.7960\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Learning Rate: 0.01, Hidden Channels: 32\n",
      "\n",
      "Test Accuracy for GCN: 0.8140\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Learning Rate: 0.01, Hidden Channels: 64\n",
      "\n",
      "Test Accuracy for GCN: 0.8030\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Learning Rate: 0.1, Hidden Channels: 32\n",
      "\n",
      "Test Accuracy for GCN: 0.8100\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Learning Rate: 0.1, Hidden Channels: 64\n",
      "\n",
      "Test Accuracy for GCN: 0.8030\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "hidden_channels = [32, 64]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hidden_channel in hidden_channels:\n",
    "        print(f\"\\nLearning Rate: {lr}, Hidden Channels: {hidden_channel}\\n\")\n",
    "\n",
    "        # GCN\n",
    "        gcn_model = gcn.GCN(\n",
    "            num_classes=dataset.num_classes,\n",
    "            num_features=dataset.num_features,\n",
    "            hidden_channels=hidden_channel,\n",
    "        )\n",
    "        gcn_optimizer = torch.optim.Adam(gcn_model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        loop(gcn_model, gcn_optimizer, silent=True)\n",
    "\n",
    "        # # GraphSAGE\n",
    "        # graphsage_model = graphsage.SAGE(\n",
    "        #     num_classes=dataset.num_classes,\n",
    "        #     num_features=dataset.num_features,\n",
    "        #     hidden_channels=hidden_channel,\n",
    "        # )\n",
    "        # graphsage_optimizer = torch.optim.Adam(\n",
    "        #     graphsage_model.parameters(), lr=lr, weight_decay=5e-4\n",
    "        # )\n",
    "        # loop(graphsage_model, graphsage_optimizer, silent=True)\n",
    "\n",
    "        # # GraphSAINT\n",
    "        # graphsaint_model = graphsaint.SAINT(\n",
    "        #     num_classes=dataset.num_classes,\n",
    "        #     num_node_features=dataset.num_node_features,\n",
    "        #     hidden_channels=hidden_channel,\n",
    "        # )\n",
    "        # graphsaint_optimizer = torch.optim.Adam(\n",
    "        #     graphsaint_model.parameters(), lr=lr, weight_decay=5e-4\n",
    "        # )\n",
    "        # loop(graphsaint_model, graphsaint_optimizer, silent=True)\n",
    "\n",
    "        # # GAT\n",
    "        # gat_model = gat.GAT(\n",
    "        #     num_classes=dataset.num_classes,\n",
    "        #     num_features=dataset.num_features,\n",
    "        #     hidden_channels=hidden_channel,\n",
    "        # )\n",
    "        # gat_optimizer = torch.optim.Adam(gat_model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        # loop(gat_model, gat_optimizer, silent=True)\n",
    "\n",
    "        # # MLP\n",
    "        # mlp_model = mlp.MLP(\n",
    "        #     num_classes=dataset.num_classes,\n",
    "        #     num_features=dataset.num_features,\n",
    "        #     hidden_channels=hidden_channel,\n",
    "        # )\n",
    "        # mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        # loop(mlp_model, mlp_optimizer, silent=True)\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "from models import graphmlp\n",
    "\n",
    "def train_graphmlp(model, optimizer, data, criterion=torch.nn.CrossEntropyLoss(), tau=1.0, alpha=1.0, k_hop=2):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out, x_dis = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss_train_class = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    adj_label = model_utils.get_A_r(data, k_hop)\n",
    "    loss_Ncontrast = model_utils.Ncontrast(x_dis, adj_label, tau=tau)\n",
    "    loss_train = loss_train_class + loss_Ncontrast * alpha\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_train\n",
    "\n",
    "\n",
    "def train(model, optimizer, data, criterion=torch.nn.CrossEntropyLoss()):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [2708] at index 0 does not match the shape of the indexed tensor [2166, 32] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_graphmlp(graphmlp_model, graphmlp_optimizer, dataset[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;32m/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m out, x_dis \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[data\u001b[39m.\u001b[39;49mtrain_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss_train_class \u001b[39m=\u001b[39m criterion(out[data\u001b[39m.\u001b[39mtrain_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m adj_label \u001b[39m=\u001b[39m model_utils\u001b[39m.\u001b[39mget_A_r(data, k_hop)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [2708] at index 0 does not match the shape of the indexed tensor [2166, 32] at index 0"
     ]
    }
   ],
   "source": [
    "train_graphmlp(graphmlp_model, graphmlp_optimizer, dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Model.__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m graphmlp \u001b[39mas\u001b[39;00m gmlp\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m gmlp\u001b[39m.\u001b[39;49mModel()\n",
      "\u001b[0;31mTypeError\u001b[0m: Model.__init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "from model import graphmlp as gmlp\n",
    "\n",
    "model = gmlp.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GMLP' from 'models' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m GMLP\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_citation, accuracy, get_A_r\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tsimiho/Desktop/NTUA/ECE/DiplomaThesis/code/repo/pipeline/training_notebook.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Settings\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GMLP' from 'models' (unknown location)"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import GMLP\n",
    "from utils import load_citation, accuracy, get_A_r\n",
    "\n",
    "# Settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--epochs', type=int, default=400,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=256,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--data', type=str, default='cora',\n",
    "                    help='dataset to be used')\n",
    "parser.add_argument('--alpha', type=float, default=2.0,\n",
    "                    help='To control the ratio of Ncontrast loss')\n",
    "parser.add_argument('--batch_size', type=int, default=2048,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--order', type=int, default=2,\n",
    "                    help='to compute order-th power of adj')\n",
    "parser.add_argument('--tau', type=float, default=1.0,\n",
    "                    help='temperature for Ncontrast loss')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "## get data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_citation(args.data, 'AugNormAdj', True)\n",
    "adj_label = get_A_r(adj, args.order)\n",
    "\n",
    "\n",
    "## Model and optimizer\n",
    "model = GMLP(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout,\n",
    "            )\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def Ncontrast(x_dis, adj_label, tau = 1):\n",
    "    \"\"\"\n",
    "    compute the Ncontrast loss\n",
    "    \"\"\"\n",
    "    x_dis = torch.exp( tau * x_dis)\n",
    "    x_dis_sum = torch.sum(x_dis, 1)\n",
    "    x_dis_sum_pos = torch.sum(x_dis*adj_label, 1)\n",
    "    loss = -torch.log(x_dis_sum_pos * (x_dis_sum**(-1))+1e-8).mean()\n",
    "    return loss\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    \"\"\"\n",
    "    get a batch of feature & adjacency matrix\n",
    "    \"\"\"\n",
    "    rand_indx = torch.tensor(np.random.choice(np.arange(adj_label.shape[0]), batch_size)).type(torch.long).cuda()\n",
    "    rand_indx[0:len(idx_train)] = idx_train\n",
    "    features_batch = features[rand_indx]\n",
    "    adj_label_batch = adj_label[rand_indx,:][:,rand_indx]\n",
    "    return features_batch, adj_label_batch\n",
    "\n",
    "def train():\n",
    "    features_batch, adj_label_batch = get_batch(batch_size=args.batch_size)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output, x_dis = model(features_batch)\n",
    "    loss_train_class = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    loss_Ncontrast = Ncontrast(x_dis, adj_label_batch, tau = args.tau)\n",
    "    loss_train = loss_train_class + loss_Ncontrast * args.alpha\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    return \n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    return acc_test, acc_val\n",
    "\n",
    "best_accu = 0\n",
    "best_val_acc = 0\n",
    "print('\\n'+'training configs', args)\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    train()\n",
    "    tmp_test_acc, val_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Summary: tensor([0.1397, 0.1057, 0.1681, 0.2023, 0.1595, 0.1230, 0.1016])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "gat_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = gat_model(dataset[0])\n",
    "    \n",
    "gs = F.softmax(output, dim=1).mean(dim=0)\n",
    "\n",
    "print(\"Graph Summary:\", gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GMN_PyTorch import models\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.feat_dim = 64 \n",
    "        self.dim = 128     \n",
    "        self.num_layers = 3\n",
    "        self.n_classes = 2 \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.logging = True\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmn = models.GraphMatchingNetwork(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
