{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from evaluation import compute_similarity, auc\n",
    "from loss import pairwise_loss, triplet_loss\n",
    "from gmn_utils import *\n",
    "from configure import *\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GATConv, dense_mincut_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import torch\n",
    "from torch_geometric.nn import dense_mincut_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATWithMinCutPooling(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        num_classes,\n",
    "        num_hidden=8,\n",
    "        heads=8,\n",
    "        dropout=0.6,\n",
    "        name=\"GAT\",\n",
    "    ):  \n",
    "        super(GATWithMinCutPooling, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            in_channels=num_features,\n",
    "            out_channels=num_hidden,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            in_channels=num_hidden * heads,\n",
    "            out_channels=num_classes,\n",
    "            heads=1,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.feature_transform = torch.nn.Linear(num_classes, num_features)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = F.dropout(data.x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, data.edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        \n",
    "        x_dense, mask = to_dense_batch(data.x, data.batch)\n",
    "        adj_dense = to_dense_adj(data.edge_index, data.batch)\n",
    "\n",
    "        num_clusters = dataset.num_classes\n",
    "\n",
    "        transform = torch.nn.Linear(x.size(1), num_clusters)\n",
    "        s = transform(x)\n",
    "        s = torch.softmax(s, dim=1)\n",
    "\n",
    "        x_pool, adj_pool, mincut_loss, ortho_loss = dense_mincut_pool(x_dense, adj_dense, s, mask=mask)\n",
    "        x_pool = x_pool.squeeze(0)\n",
    "\n",
    "        return x, x_pool, adj_pool, mincut_loss, ortho_loss\n",
    "\n",
    "classification_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_class, x_pool, adj_pool, mincut_loss, ortho_loss = model(data)\n",
    "    classification_loss = classification_criterion(x_class[data.train_mask], data.y[data.train_mask])\n",
    "    loss = classification_loss + mincut_loss + ortho_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), classification_loss, mincut_loss, ortho_loss\n",
    "\n",
    "def validate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_class, x_pool, adj_pool, mincut_loss, ortho_loss = model(data)\n",
    "        classification_loss = classification_criterion(x_class[data.val_mask], data.y[data.val_mask])\n",
    "        loss = classification_loss + mincut_loss + ortho_loss\n",
    "        return loss.item(), classification_loss, mincut_loss, ortho_loss\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_class, x_pool, adj_pool, mincut_loss, ortho_loss = model(data)\n",
    "        classification_loss = classification_criterion(x_class[data.test_mask], data.y[data.test_mask])\n",
    "        loss = classification_loss + mincut_loss + ortho_loss\n",
    "        return loss.item(), classification_loss, mincut_loss, ortho_loss\n",
    "\n",
    "dataset = Planetoid(root='../Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "classification_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x_class, x_pool, adj_pool, mincut_loss, ortho_loss = model(data)\n",
    "    classification_loss = classification_criterion(x_class[data.train_mask], data.y[data.train_mask])\n",
    "    loss = classification_loss + mincut_loss + ortho_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), classification_loss, mincut_loss, ortho_loss\n",
    "\n",
    "def validate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_class, x_pool, adj_pool, mincut_loss, ortho_loss = model(data)\n",
    "        classification_loss = classification_criterion(x_class[data.val_mask], data.y[data.val_mask])\n",
    "        loss = classification_loss + mincut_loss + ortho_loss\n",
    "        return loss.item(), classification_loss, mincut_loss, ortho_loss\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_class, x_pool, adj_pool, mincut_loss, ortho_loss = model(data)\n",
    "        classification_loss = classification_criterion(x_class[data.test_mask], data.y[data.test_mask])\n",
    "        loss = classification_loss + mincut_loss + ortho_loss\n",
    "        return loss.item(), classification_loss, mincut_loss, ortho_loss\n",
    "\n",
    "dataset = Planetoid(root='../Cora', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7703790068626404\n",
      "\ttest_class_loss = 0.6601805686950684, test_mincut_loss = -0.9986997246742249, test_ortho_loss = 1.1088981628417969\n"
     ]
    }
   ],
   "source": [
    "final_model = GATWithMinCutPooling(num_features=dataset.num_features, num_classes=dataset.num_classes, dropout=0.6)\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=0.005)    \n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss, train_class_loss, train_mincut_loss, train_ortho_loss = train(final_model, data, optimizer)\n",
    "             \n",
    "test_loss, test_class_loss, test_mincut_loss, test_ortho_loss = test(final_model, data)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f'\\ttest_class_loss = {test_class_loss}, test_mincut_loss = {test_mincut_loss}, test_ortho_loss = {test_ortho_loss}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = final_model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out, x_pool, adj_pool, mincut_loss, ortho_loss = model(data)\n",
    "\n",
    "edge_index_pool = dense_to_sparse(adj_pool)[0]\n",
    "\n",
    "pooled_data = Data(x=x_pool, edge_index=edge_index_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = Data(x=data.x, edge_index=data.edge_index)\n",
    "g2 = pooled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder= {'node_hidden_sizes': [32], 'node_feature_dim': 1, 'edge_hidden_sizes': [16]}\n",
      "aggregator= {'node_hidden_sizes': [128], 'graph_transform_sizes': [128], 'input_size': [32], 'gated': True, 'aggregation_type': 'sum'}\n",
      "graph_embedding_net= {'node_state_dim': 32, 'edge_state_dim': 16, 'edge_hidden_sizes': [64, 64], 'node_hidden_sizes': [64], 'n_prop_layers': 5, 'share_prop_params': True, 'edge_net_init_scale': 0.1, 'node_update_type': 'gru', 'use_reverse_direction': True, 'reverse_dir_param_different': False, 'layer_norm': False, 'prop_type': 'matching'}\n",
      "graph_matching_net= {'node_state_dim': 32, 'edge_state_dim': 16, 'edge_hidden_sizes': [64, 64], 'node_hidden_sizes': [64], 'n_prop_layers': 5, 'share_prop_params': True, 'edge_net_init_scale': 0.1, 'node_update_type': 'gru', 'use_reverse_direction': True, 'reverse_dir_param_different': False, 'layer_norm': False, 'prop_type': 'matching', 'similarity': 'dotproduct'}\n",
      "model_type= matching\n",
      "data= {'problem': 'graph_edit_distance', 'dataset_params': {'n_nodes_range': [20, 20], 'p_edge_range': [0.2, 0.2], 'n_changes_positive': 1, 'n_changes_negative': 2, 'validation_dataset_size': 1000}}\n",
      "training= {'batch_size': 20, 'learning_rate': 0.0001, 'mode': 'pair', 'loss': 'cosine', 'margin': 1.0, 'graph_vec_regularizer_weight': 1e-06, 'clip_value': 10.0, 'n_training_steps': 100000, 'print_after': 10, 'eval_after': 10}\n",
      "evaluation= {'batch_size': 1}\n",
      "seed= 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphMatchingNet(\n",
       "  (_encoder): GraphEncoder(\n",
       "    (MLP1): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=32, bias=True)\n",
       "    )\n",
       "    (MLP2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_aggregator): GraphAggregator(\n",
       "    (MLP1): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    )\n",
       "    (MLP2): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_prop_layers): ModuleList(\n",
       "    (0-4): 5 x GraphPropMatchingLayer(\n",
       "      (_message_net): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (_reverse_message_net): Sequential(\n",
       "        (0): Linear(in_features=80, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (GRU): GRU(96, 32)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Print configure\n",
    "config = get_default_config()\n",
    "for k, v in config.items():\n",
    "    print(\"%s= %s\" % (k, v))\n",
    "\n",
    "# Set random seeds\n",
    "seed = config[\"seed\"]\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "training_set, validation_set = build_datasets(config)\n",
    "\n",
    "if config[\"training\"][\"mode\"] == \"pair\":\n",
    "    training_data_iter = training_set.pairs(config[\"training\"][\"batch_size\"])\n",
    "    first_batch_graphs, _ = next(training_data_iter)\n",
    "else:\n",
    "    training_data_iter = training_set.triplets(config[\"training\"][\"batch_size\"])\n",
    "    first_batch_graphs = next(training_data_iter)\n",
    "\n",
    "node_feature_dim = first_batch_graphs.node_features.shape[-1]\n",
    "edge_feature_dim = first_batch_graphs.edge_features.shape[-1]\n",
    "\n",
    "model, optimizer = build_model(config, node_feature_dim, edge_feature_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/st/cyh3v8ln22sdswp1s386w5vw0000gn/T/ipykernel_53819/3761369209.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from_idx = torch.tensor(from_idx, dtype=torch.long)\n",
      "/var/folders/st/cyh3v8ln22sdswp1s386w5vw0000gn/T/ipykernel_53819/3761369209.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  to_idx = torch.tensor(to_idx, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "reduced_node_features = pca.fit_transform(g1.x)\n",
    "\n",
    "from_idx = g1.edge_index[0]\n",
    "to_idx = g1.edge_index[1]\n",
    "\n",
    "num_edges = g1.edge_index.shape[1]\n",
    "\n",
    "graph_idx = np.zeros(g1.x.shape[0], dtype=int)\n",
    "\n",
    "node_features = torch.tensor(reduced_node_features, dtype=torch.float32)\n",
    "edge_features = torch.ones((num_edges, 4), dtype=torch.float32)\n",
    "from_idx = torch.tensor(from_idx, dtype=torch.long)\n",
    "to_idx = torch.tensor(to_idx, dtype=torch.long)\n",
    "graph_idx = torch.tensor(graph_idx, dtype=torch.long)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "stacked_node_features = torch.cat((node_features, node_features.clone()))\n",
    "stacked_edge_features = torch.cat((edge_features, edge_features.clone()))\n",
    "\n",
    "stacked_from_idx = torch.cat((from_idx, from_idx.clone()+g1.x.shape[0]))\n",
    "stacked_to_idx = torch.cat((to_idx, to_idx.clone()+g1.x.shape[0]))\n",
    "\n",
    "stacked_graph_idx = torch.cat((torch.tensor(np.zeros(g1.x.shape[0], dtype=int)), torch.tensor(np.ones(g1.x.shape[0], dtype=int))))\n",
    "\n",
    "graph_vectors = model(\n",
    "        stacked_node_features.to(device),\n",
    "        stacked_edge_features.to(device),\n",
    "        stacked_from_idx.to(device), \n",
    "        stacked_to_idx.to(device),\n",
    "        stacked_graph_idx.to(device),\n",
    "        2\n",
    "    )\n",
    "\n",
    "x, y = reshape_and_split_tensor(graph_vectors, 2)\n",
    "similarity = compute_similarity(config, x, y)\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
